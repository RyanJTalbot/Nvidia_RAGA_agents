{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dace1a29-864a-4a69-bd00-c0952a73d4ea",
   "metadata": {
    "id": "dace1a29-864a-4a69-bd00-c0952a73d4ea"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TONXsdAOC5Dn",
   "metadata": {
    "id": "TONXsdAOC5Dn"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 4:** Running State Chains</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "In the previous notebook, we introduced some key LangChain Expression Language (LCEL) material regarding runnables. By now, you should be comfortable with both internal and external reasoning, as well as how to develop pipelines that facilitate it! In this notebook, we will make our way towards more advanced paradigms that will allow us to orchestrate more complex dialog management strategies and begin to execute on longer-form document reasoning.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Learning how to leverage runnables to orchestrate interesting LLM systems.  \n",
    "- Understanding how running state chains can be used for dialog management and iterative decision-making.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- Would there ever be a use for a single-module variant of the running state chain that is not constantly querying the environment for input?\n",
    "- You may notice that the JSON prediction is actually working pretty well. It might not always work so well depending on the questions and the JSON format complexity. What kinds of issues do you expect to encounter in this regard?\n",
    "- What kinds of approaches can you think of completely swapping prompts as part of the running state chain?\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Notebook Source:**\n",
    "\n",
    "- This notebook is part of a larger [**NVIDIA Deep Learning Institute**](https://www.nvidia.com/en-us/training/) course titled [**Building RAG Agents with LLMs**](https://learn.next.courses.nvidia.com/courses/course-v1:DLI+S-FX-15+V1/about). If sharing this material, please give credit and link back to the original course.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "### **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5AbHxz61Hq9x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44079,
     "status": "ok",
     "timestamp": 1703083363284,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "5AbHxz61Hq9x",
    "outputId": "b50f4635-f16e-44f9-da54-fdf108c83445"
   },
   "outputs": [],
   "source": [
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio\n",
    "\n",
    "## If you're in colab and encounter a typing-extensions issue,\n",
    "##  restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints._common import NVEModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "XTqJSv0kHtCj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44113,
     "status": "ok",
     "timestamp": 1703083407333,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "XTqJSv0kHtCj",
    "outputId": "6a37e32c-1e51-4f90-c9c7-dddc75c2b2eb"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "NVIDIA API Key:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved NVIDIA_API_KEY beginning with \"nvapi-uCz...\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'playground_nemotron_qa_8b': '0c60f14d-46cb-465e-b994-227e1c3d5047',\n",
       " 'playground_smaug_72b': '008cff6d-4f4c-4514-b61e-bcfad6ba52a7',\n",
       " 'ai-embed-qa-4': '09c64e32-2b65-4892-a285-2f585408d118',\n",
       " 'playground_llama2_70b': '0e349b44-440a-44e1-93e9-abe8dcb27158',\n",
       " 'playground_nvolveqa_40k': '091a03bb-7364-4087-8090-bd71e9277520',\n",
       " 'playground_mistral_7b': '35ec3354-2681-4d0e-a8dd-80325dcf7c63',\n",
       " 'playground_gemma_7b': '1361fa56-61d7-4a12-af32-69a3825746fa',\n",
       " 'playground_llama2_code_70b': '2ae529dc-f728-4a46-9b8d-2697213666d8',\n",
       " 'ai-rerank-qa-mistral-4b': '0bf77f50-5c35-4488-8e7a-f49bb1974af6',\n",
       " 'ai-llama2-70b': '2fddadfb-7e76-4c8a-9b82-f7d3fab94471',\n",
       " 'ai-parakeet-ctc-riva': '22164014-a6cc-4a6f-b048-f3a303e745bb',\n",
       " 'playground_nemotron_steerlm_8b': '1423ff2f-d1c7-4061-82a7-9e8c67afd43a',\n",
       " 'playground_starcoder2_15b': '6acada03-fe2f-4e4d-9e0a-e711b9fd1b59',\n",
       " 'playground_phi2': '6251d6d2-54ee-4486-90f4-2792bf0d3acd',\n",
       " 'playground_yi_34b': '347fa3f3-d675-432c-b844-669ef8ee53df',\n",
       " 'ai-microsoft-kosmos-2': '6018fed7-f227-48dc-99bc-3fd4264d5037',\n",
       " 'playground_sdxl': '89848fb8-549f-41bb-88cb-95d6597044a4',\n",
       " 'playground_gemma_2b': '5bde8f6f-7e83-4413-a0f2-7b97be33988e',\n",
       " 'ai-molmim-generate': '72be0b68-179f-412c-ac03-9a481f78cb9f',\n",
       " 'playground_seamless': '72ad9555-2e3d-4e73-9050-a37129064743',\n",
       " 'playground_deplot': '3bc390c7-eeec-40f7-a64d-0c6a719985f7',\n",
       " 'playground_clip': '8c21289c-0b18-446d-8838-011b7249c513',\n",
       " 'playground_neva_22b': '8bf70738-59b9-4e5f-bc87-7ab4203be7a0',\n",
       " 'playground_nv_llama2_rlhf_70b': '7b3e3361-4266-41c8-b312-f5e33c81fc92',\n",
       " 'ai-vista-3d': '72311276-923f-4478-a506-d5b80914728a',\n",
       " 'ai-google-deplot': '784a8ca4-ea7d-4c93-bb46-ec027c3fae47',\n",
       " 'playground_mixtral_8x7b': '8f4118ba-60a8-4e6b-8574-e38a4067a4a3',\n",
       " 'ai-stable-video-diffusion': '8cd594f1-6a4d-4f8f-82b4-d1bf89adae98',\n",
       " 'playground_cuopt': '8f2fbd00-2633-41ce-ab4e-e5736d74bff7',\n",
       " 'playground_fuyu_8b': '9f757064-657f-4c85-abd7-37a7a9b6ee11',\n",
       " 'ai-mixtral-8x7b-instruct': 'a1e53ece-bff4-44d1-8b13-c009e5bf47f6',\n",
       " 'playground_llama_guard': 'b34280ac-24e4-4081-bfaa-501e9ee16b6f',\n",
       " 'ai-gemma-7b': 'a13e3bed-ca42-48f8-b3f1-fbc47b9675f9',\n",
       " 'ai-esmfold': 'a68c59e0-47a6-4a50-bf64-6d88766d56bf',\n",
       " 'ai-stable-diffusion-xl': 'c1b63bb0-448b-4e53-b2a7-fb0b3723cbe2',\n",
       " 'playground_steerlm_llama_70b': 'd6fe6881-973a-4279-a0f8-e1d486c9618d',\n",
       " 'ai-neva-22b': 'bc205f8e-1740-40df-8d32-c4321763498a',\n",
       " 'ai-nvidia-cuopt': 'b0ac1378-3d00-43cb-a8d9-0f0c37ef36c0',\n",
       " 'ai-fuyu-8b': 'e598bfc1-b058-41af-869d-556d3c7e1b48',\n",
       " 'playground_llama2_13b': 'e0bb7fb9-5333-4a27-8534-c6288f921d3f',\n",
       " 'playground_llama2_code_13b': 'f6a96af4-8bf9-4294-96d6-d71aa787612e',\n",
       " 'ai-diffdock': 'f3dda972-561a-4772-8c09-873594b6fb72',\n",
       " 'ai-mistral-7b-instruct-v2': 'd7618e99-db93-4465-af4d-330213a7f51f',\n",
       " 'playground_llama2_code_34b': 'df2bee43-fb69-42b9-9ee5-f4eabbeaf3a8',\n",
       " 'playground_kosmos_2': '0bcd1a8c-451f-4b12-b7f0-64b4781190d1',\n",
       " 'ai-sdxl-turbo': 'f886140c-424e-4c82-a841-99e23f9ae35d',\n",
       " 'ai-codellama-70b': 'f6b06895-d073-4714-8bb2-26c09e9f6597',\n",
       " 'playground_mamba_chat': '381be320-4721-4664-bd75-58f8783b43c7'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import requests\n",
    "import os\n",
    "\n",
    "hard_reset = False  ## <-- Set to True if you want to reset your NVIDIA_API_KEY\n",
    "while \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\") or hard_reset:\n",
    "    try: \n",
    "        assert not hard_reset\n",
    "        response = requests.get(\"http://docker_router:8070/get_key\").json()\n",
    "        assert response.get('nvapi_key')\n",
    "    except: response = {'nvapi_key' : getpass(\"NVIDIA API Key: \")}\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = response.get(\"nvapi_key\")\n",
    "    try: requests.post(\"http://docker_router:8070/set_key/\", json={'nvapi_key' : os.environ[\"NVIDIA_API_KEY\"]}).json()\n",
    "    except: pass\n",
    "    hard_reset = False\n",
    "    if \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\"):\n",
    "        print(\"[!] API key assignment failed. Make sure it starts with `nvapi-` as generated from the model pages.\")\n",
    "\n",
    "print(f\"Retrieved NVIDIA_API_KEY beginning with \\\"{os.environ.get('NVIDIA_API_KEY')[:9]}...\\\"\")\n",
    "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
    "NVEModel().available_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2e4ef-6e3a-4796-b51e-c9da0c156ded",
   "metadata": {
    "id": "0bf2e4ef-6e3a-4796-b51e-c9da0c156ded"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1:** Keeping Variables Flowing\n",
    "\n",
    "In the previous examples, we were able to implement interesting logic in our standalone chains by **creating**, **mutating**, and **consuming** states. These states were passed around as dictionaries with descriptive keys and useful values, and the values would be used to supply follow-up routines with the info they need to operate!\n",
    "\n",
    "**Recall the zero-shot classification example from the last notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aa215f6-050c-4062-a455-feffd77daaeb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5829,
     "status": "ok",
     "timestamp": 1703083414406,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "7aa215f6-050c-4062-a455-feffd77daaeb",
    "outputId": "df76d362-e2b9-437c-910f-aa1127cd3f44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "of the given options are relevant to the question. Therefore, no answer will be provided. However, if you need help with transportation-related topics, please choose one from the options: car, boat, airplane, or bike.\n",
      "--------------------------------------------------------------------------------\n",
      "boat\n",
      "--------------------------------------------------------------------------------\n",
      "(Since the fear is specifically about heights in the context of traveling, the option that best fits is airplanes.)\n",
      "CPU times: user 1.34 s, sys: 2.1 s, total: 3.44 s\n",
      "Wall time: 5.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## ^^ This notebook is timed, which will print out how long it all took\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import List, Union\n",
    "from operator import itemgetter\n",
    "\n",
    "## Zero-shot classification prompt and chain\n",
    "zsc_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"One word answer. Output the most relevant topic option. [Options : {options}]\"\n",
    "    )),\n",
    "    (\"user\", \"{input} = \")\n",
    "])\n",
    "\n",
    "## Useful method for mistral, which is currently tuned to output numbered outputs\n",
    "def EnumParser(*idxs):\n",
    "    '''Method that pulls out values from a mistral model that outputs numbered entries'''\n",
    "    idxs = idxs or [slice(0, None, 1)]\n",
    "    entry_parser = lambda v: v if (' ' not in v) else v[v.index(' '):]\n",
    "    out_lambda = lambda x: [entry_parser(v).strip() for v in x.split(\"\\n\")]\n",
    "    return StrOutputParser() | RunnableLambda(lambda x: itemgetter(*idxs)(out_lambda(x)))\n",
    "\n",
    "## Define your simple instruct_model\n",
    "instruct_llm = ChatNVIDIA(model=\"mistral_7b\") | EnumParser(0)  ## TODO: Try out some more models\n",
    "\n",
    "zsc_chain = zsc_prompt | instruct_llm\n",
    "\n",
    "def zsc_call(input, options=[\"car\", \"boat\", \"airplane\", \"bike\"]):\n",
    "    return zsc_chain.invoke({\"input\" : input, \"options\" : options})\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(zsc_call(\"Should I take the next exit, or keep going to the next one?\"))\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(zsc_call(\"I get seasick, so I think I'll pass on the trip\"))\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(zsc_call(\"I'm scared of heights, so flying probably isn't for me\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4HYI7SIxLs5W",
   "metadata": {
    "id": "4HYI7SIxLs5W"
   },
   "source": [
    "<br>\n",
    "\n",
    "This chain makes several design decisions that make it very easy to use, key among them the following:\n",
    "\n",
    "**We want it to act like a function, so all we want it to do is generate the output and return it.**\n",
    "\n",
    "This makes the chain extremely natural for inclusion as a module in a larger chain system. For example, the following chain will take a string, extract the most likely topic, and then generate a new sentence based on the topic:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "LuldQwa_PQNS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "executionInfo": {
     "elapsed": 3827,
     "status": "ok",
     "timestamp": 1703083418214,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "LuldQwa_PQNS",
    "outputId": "cdca3240-8037-4b31-e336-02a271017b6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 314 ms, sys: 13.1 ms, total: 328 ms\n",
      "Wall time: 3.52 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'on a boat under the golden sunset, feeling the gentle breeze in your hair and the cool water against your feet, is a moment of pure tranquility and freedom.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "## ^^ This notebook is timed, which will print out how long it all took\n",
    "\n",
    "gen_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Make a new sentence about the the following topic: {topic}. Be creative!\"\n",
    ")\n",
    "\n",
    "gen_chain = gen_prompt | instruct_llm\n",
    "\n",
    "input_msg = \"I get seasick, so I think I'll pass on the trip\"\n",
    "options = [\"car\", \"boat\", \"airplane\", \"bike\"]\n",
    "\n",
    "({'topic' : zsc_chain} | gen_chain).invoke({\"input\" : input_msg, \"options\" : options})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1zzDhPsfXRhn",
   "metadata": {
    "id": "1zzDhPsfXRhn"
   },
   "source": [
    "<br>\n",
    "\n",
    "However, it's a bit problematic when you want to keep the information flowing since we lose the topic and input variables in generating our response. If we wanted do something with both the output and the input, we'd need a to make sure that both variables pass through.\n",
    "\n",
    "Lucky for us, we can use the mapping runnable (i.e. interpretted from a dictionary or using manual `RunnableMap`) to pass both of the variables through by assigning the output of our chain to just a single key and letting the other keys propagate as desired. Alternatively, we could also use `RunnableAssign` to merge the state-consuming chain's output with the input dictionary by default.\n",
    "\n",
    "With this technique, we can propagate whatever we want through our chain system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "MfXm7oT5XVOd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1703083424179,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "MfXm7oT5XVOd",
    "outputId": "895b237d-dd15-4c7c-8d0e-b25be664e4a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: {'input': \"I get seasick, so I think I'll pass on the trip\", 'options': ['car', 'boat', 'airplane', 'bike', 'unknown']}\n",
      "State: {'input': \"I get seasick, so I think I'll pass on the trip\", 'topic': 'boat'}\n",
      "State: {'input': \"I get seasick, so I think I'll pass on the trip\", 'topic': 'boat', 'generation': 'on a boat under the golden sunset, feeling the gentle breeze in your hair and the cool water against your feet, is a moment of pure tranquility and freedom.'}\n",
      "CPU times: user 520 ms, sys: 0 ns, total: 520 ms\n",
      "Wall time: 4.26 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"I get seasick, so I think I'll pass on the trip\",\n",
       " 'topic': 'boat',\n",
       " 'generation': 'on a boat under the golden sunset, feeling the gentle breeze in your hair and the cool water against your feet, is a moment of pure tranquility and freedom.',\n",
       " 'combination': 'you get seasick, you might miss out on the tranquil and freeing experience of being on a boat under the golden sunset with the gentle breeze in your hair and cool water against your feet.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "## ^^ This notebook is timed, which will print out how long it all took\n",
    "\n",
    "from langchain.schema.runnable import RunnableBranch, RunnablePassthrough\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from functools import partial\n",
    "\n",
    "def RPrint(preface=\"\"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        print(f\"{preface}{x}\")\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "big_chain = (\n",
    "    RPrint(\"State: \")\n",
    "    ## Manual mapping. Can be useful sometimes and inside branch chains\n",
    "    | {'input' : lambda d: d.get('input'), 'topic' : zsc_chain}\n",
    "    | RPrint(\"State: \")\n",
    "    ## RunnableAssign passing. Better for running state chains by default\n",
    "    | RunnableAssign({'generation' : gen_chain})\n",
    "    | RPrint(\"State: \")\n",
    "    ## Using the input and generation together\n",
    "    | RunnableAssign({'combination' : (\n",
    "        ChatPromptTemplate.from_template(\n",
    "            \"Consider the following passages:\"\n",
    "            \"\\nP1: {input}\"\n",
    "            \"\\nP2: {generation}\"\n",
    "            \"\\n\\nCombine the ideas from both sentences into one simple one.\"\n",
    "        )\n",
    "        | instruct_llm\n",
    "    )})\n",
    ")\n",
    "\n",
    "big_chain.invoke({\n",
    "    \"input\" : \"I get seasick, so I think I'll pass on the trip\",\n",
    "    \"options\" : [\"car\", \"boat\", \"airplane\", \"bike\", \"unknown\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tHONiEtGK2qv",
   "metadata": {
    "id": "tHONiEtGK2qv"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** Running State Chain\n",
    "\n",
    "The example above is just a toy example and, if anything, showcases the drawbacks of chaining many LLM calls together for internal under-the-hood reasoning. However, the ability to keep information flowing through a chain is invaluable for making complex chains that can accumulate useful state information or operate in a multi-pass capacity.\n",
    "\n",
    "Specifically, a very simple but effective chain is a **Running State Chain** which enforces the following properties:\n",
    "- A **\"running state\"** is a dictionary that contains all of the variables that the system cares about.\n",
    "- A **\"branch\"** is a chain that can pull in the running state and can degenerate it into a response.\n",
    "- A **branch** can only be ran inside a **RunnableAssign** scope, and the branchs' inputs should come from the **running state**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nGOBMbF-phA6",
   "metadata": {
    "id": "nGOBMbF-phA6"
   },
   "source": [
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/running_state_chain.png\" width=1000px/>\n",
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1Oo7AauYGj4dxepNReRG2JezmvQLyqXsN\" width=1000px/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70M-ZOpMpinO",
   "metadata": {
    "id": "70M-ZOpMpinO"
   },
   "source": [
    "You can think of the running state chain abstraction as a functional variant of a Pythonic class with state variables (or attributes) and functions (or methods).\n",
    "- The chain is like the abstract class that wraps all of the functionality.\n",
    "- The running state are like the attributes (which should always be accessible).\n",
    "- The branches are like the class methods (which can pick and choose which attributes to use).\n",
    "- The `.invoke` or similar process is like the `__call__` method that runs through the branches in order.\n",
    "\n",
    "**By forcing this paradigm in your chains:**\n",
    "- You can keep state variables propagating through your chain, allowing your internals to access whatever is necessary and accumulating state values for use later.\n",
    "- You can also pass the outputs of your chain back through as your inputs, allowing a \"while-loop\"-style chain that keeps updating and building on your running state.\n",
    "\n",
    "The rest of this notebook will include two exercises that flesh out the running state chain abstraction for two additional use-cases: **Knowledge Bases** and **Database-Querying Chatbots**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "--3BDo7-wFBF",
   "metadata": {
    "id": "--3BDo7-wFBF"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3:** Implementing a Knowledge Base with Running State Chain\n",
    "\n",
    "After understanding the basic structure and principles of a Running State Chain, we can explore how this approach can be extended to manage more complex tasks, particularly in creating dynamic systems that evolve through interaction. This section will focus on implementing a **knowledge base** accumulated using **json-enabled slot filling**:\n",
    "\n",
    "- **Knowledge Base:** A store of information that's relevant for our LLM to keep track of.\n",
    "- **JSON-Enabled Slot Filling:** The technique of asking an instruction-tuned model to output a json-style format (which can include a dictionary) with a selection of slots, relying on the LLM to fill these slots with useful and relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imHxfz52wSgY",
   "metadata": {
    "id": "imHxfz52wSgY"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### **Defining Our Knowledge Base**\n",
    "\n",
    "To build a responsive and intelligent system, we need a method that not only processes inputs but also retains and updates essential information through the flow of conversation. This is where the combination of LangChain and Pydantic becomes pivotal. [**Pydantic**](https://docs.pydantic.dev/latest/), a popular Python validation library, is instrumental in structuring and validating data models. As one of its features, Pydantic offers structured \"model\" classes that validate objects (data, classes, themselves, etc.) with simplified syntax and deep rabbitholes of customization options. This framework is used throughout LangChain and comes up as a necessary component for use cases that involve data coersion.\n",
    "\n",
    "One thing that a \"model\" is very good for is defining a class with expected arguments and some special ways to validate them! In this course, we won't focus too much on the validation scripts, but those interested can start by checking out the [**Pydantic Validator guide**](https://docs.pydantic.dev/1.10/usage/validators/) (though the topics do get pretty deep pretty fast). For our purposes, we can construct a `BaseModel` class and define some `Field` variables to create a structured **Knowledge Base** like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "WWaYkSP9wQaF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1703083424179,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "WWaYkSP9wQaF",
    "outputId": "76d4c995-4423-478b-9c66-01b48a27c866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeBase(topic='Travel', user_preferences={}, session_notes='', unresolved_queries=[], action_items=[])\n"
     ]
    }
   ],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Dict, Union, Optional\n",
    "\n",
    "class KnowledgeBase(BaseModel):\n",
    "    ## Fields of the BaseModel, which will be validated/assigned when the knowledge base is constructed\n",
    "    topic: str = Field('general', description=\"Current conversation topic\")\n",
    "    user_preferences: Dict[str, Union[str, int]] = Field({}, description=\"User preferences and choices\")\n",
    "    session_notes: str = Field(\"\", description=\"Notes on the ongoing session\")\n",
    "    unresolved_queries: list = Field([], description=\"Unresolved user queries\")\n",
    "    action_items: list = Field([], description=\"Actionable items identified during the conversation\")\n",
    "\n",
    "print(repr(KnowledgeBase(topic = \"Travel\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_g8qYKZBwYsO",
   "metadata": {
    "id": "_g8qYKZBwYsO"
   },
   "source": [
    "<br>\n",
    "\n",
    "The true strength of this approach lies in the additional LLM-centric functionalities provided by LangChain which we can integrate for our use-cases. One such feature is the `PydanticOutputParser` which enhances the Pydantic objects with capabilities like automatic format instruction generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "H6uK93zwwdcn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1703083424311,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "H6uK93zwwdcn",
    "outputId": "fb4f1a54-0a3c-46e6-fe19-4f1f9282bc61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"topic\": {\"title\": \"Topic\", \"description\": \"Current conversation topic\", \"default\": \"general\", \"type\": \"string\"}, \"user_preferences\": {\"title\": \"User Preferences\", \"description\": \"User preferences and choices\", \"default\": {}, \"type\": \"object\", \"additionalProperties\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"integer\"}]}}, \"session_notes\": {\"title\": \"Session Notes\", \"description\": \"Notes on the ongoing session\", \"default\": \"\", \"type\": \"string\"}, \"unresolved_queries\": {\"title\": \"Unresolved Queries\", \"description\": \"Unresolved user queries\", \"default\": [], \"type\": \"array\", \"items\": {}}, \"action_items\": {\"title\": \"Action Items\", \"description\": \"Actionable items identified during the conversation\", \"default\": [], \"type\": \"array\", \"items\": {}}}}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "instruct_string = PydanticOutputParser(pydantic_object=KnowledgeBase).get_format_instructions()\n",
    "print(instruct_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oFyxUZEMwp9s",
   "metadata": {
    "id": "oFyxUZEMwp9s"
   },
   "source": [
    "<br>\n",
    "\n",
    "This functionality generates instructions for creating valid inputs to the Knowledge Base, which in turn helps the LLM by providing a concrete one-shot example of the desired output format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JIGIXNfmwqAz",
   "metadata": {
    "id": "JIGIXNfmwqAz"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### **Runnable Extraction Module**\n",
    "\n",
    "Knowing that we have this Pydantic object which can be used to generate good LLM instructions, we can make a Runnable that wraps the functionality of our Pydantic class and streamlines the prompting, generating, and updating of the knowledge base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "Zya1sucNwwIO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3469,
     "status": "ok",
     "timestamp": 1703086115894,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "Zya1sucNwwIO",
    "outputId": "fb5a0b21-dcfc-4e6f-f621-a5466047fc3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeBase(topic='flowers', user_preferences={}, session_notes='User requested purchase of orchids.', unresolved_queries=[{'text': 'Can you buy me some orchids?'}], action_items=[{'text': 'Look into purchasing orchids for the user.'}])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################\n",
    "## Definition of RExtract\n",
    "def RExtract(pydantic_class, llm, prompt):\n",
    "    '''\n",
    "    Runnable Extraction module\n",
    "    Returns a knowledge dictionary populated by slot-filling extraction\n",
    "    '''\n",
    "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
    "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
    "    def preparse(string):\n",
    "        if '{' not in string: string = '{' + string\n",
    "        if '}' not in string: string = string + '}'\n",
    "        string = (string\n",
    "            .replace(\"\\\\_\", \"_\")\n",
    "            .replace(\"\\n\", \" \")\n",
    "            .replace(\"\\]\", \"]\")\n",
    "            .replace(\"\\[\", \"[\")\n",
    "        )\n",
    "        # print(string)  ## Good for diagnostics\n",
    "        return string\n",
    "    return instruct_merge | prompt | llm | preparse | parser\n",
    "\n",
    "################################################################################\n",
    "## Practical Use of RExtract\n",
    "instruct_model_big = ChatNVIDIA(model = \"mixtral_8x7b\") | StrOutputParser()\n",
    "\n",
    "parser_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Update the knowledge base: {format_instructions}. Only use information from the input.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "extractor = RExtract(KnowledgeBase, instruct_model_big, parser_prompt)\n",
    "\n",
    "knowledge = extractor.invoke({'input' : \"I love flowers so much! The orchids are amazing! Can you buy me some?\"})\n",
    "knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6Rjncqne2-zm",
   "metadata": {
    "id": "6Rjncqne2-zm"
   },
   "source": [
    "<br>\n",
    "\n",
    "Do keep in mind that this process can fail due to the fuzzy nature of LLM prediction, especially with models that are not optimized for instruction-following! For this process, it's important to have a strong instruction-following LLM with extra checks and graceful failure routines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qePT8nsdDskr",
   "metadata": {
    "id": "qePT8nsdDskr"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### **Dynamic Knowledge Base Updates**\n",
    "\n",
    "Finally, we can create a system that continually updates the Knowledge Base throughout the conversation. This is done by feeding the current state of the Knowledge Base, along with new user inputs, back into the system for ongoing updates.\n",
    "\n",
    "The following is an example system that shows off both the formulation's power of filling details as well as the limitations of assuming that filling performance will be as good as general response performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8yKtrLeBDu35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6433,
     "status": "ok",
     "timestamp": 1703083433913,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "8yKtrLeBDu35",
    "outputId": "282c71b5-8056-4115-d4c1-70254a1e9caf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'know_base': KnowledgeBase(firstname='Carmen', lastname='Sandiego', location='unknown', hints='Guess location', response=\"I'm not sure where you are, Carmen Sandiego. Please tell me the exact location.\"), 'input': 'My name is Carmen Sandiego! Guess where I am?'}\n",
      "{'know_base': KnowledgeBase(firstname='Carmen', lastname='Sandiego', location='The United States', hints='Guess location', response=\"You're right, Carmen! The United States is indeed a large country. Could you please provide a more specific location within the US?\"), 'input': 'The United States is a big place! Can you be more specific?'}\n",
      "{'know_base': KnowledgeBase(firstname='Carmen', lastname='Sandiego', location='New Orleans, The United States', hints='Guess location', response=\"I made an educated guess based on your previous response. New Orleans is a well-known city in the United States. I'm here to help, so feel free to ask any questions you have!\"), 'input': \"Yeah, I'm in New Orleans... How did you know?\"}\n"
     ]
    }
   ],
   "source": [
    "class KnowledgeBase(BaseModel):\n",
    "    firstname: str = Field('unknown', description=\"Chatting user's first name, unknown if unknown\")\n",
    "    lastname: str = Field('unknown', description=\"Chatting user's last name, unknown if unknown\")\n",
    "    location: str = Field('unknown', description=\"Where the user is located\")\n",
    "    hints: str = Field('unknown', description=\"Hints to help answer other questions\")\n",
    "    response: str = Field('unknown', description=\"Ideal response based on last user input\")\n",
    "\n",
    "\n",
    "parser_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"The user just responsed. Please update the knowledge base based on the response.\"\n",
    "        \" This information will be acted on to respond to the user in the next interaction.\"\n",
    "        \" Do not hallucinate any details, and make sure the knowledge base is not redundant.\"\n",
    "        \" Update the entries frequently to adapt to the conversation flow.\"\n",
    "        \"\\n{format_instructions}\"\n",
    "    )), (\"user\", \"CURRENT KNOWLEDGE BASE: {know_base}\\nUser: {input}\"),\n",
    "])\n",
    "\n",
    "instruct_model_big = ChatNVIDIA(model = \"mixtral_8x7b\") | StrOutputParser()\n",
    "\n",
    "extractor = RExtract(KnowledgeBase, instruct_model_big, parser_prompt)\n",
    "info_update = RunnableAssign({'know_base' : extractor})\n",
    "\n",
    "state = {'know_base' : KnowledgeBase()}\n",
    "state['input'] = \"My name is Carmen Sandiego! Guess where I am?\"\n",
    "state = info_update.invoke(state)\n",
    "print(state)\n",
    "\n",
    "state['input'] = \"The United States is a big place! Can you be more specific?\"\n",
    "state = info_update.invoke(state)\n",
    "print(state)\n",
    "\n",
    "state['input'] = \"Yeah, I'm in New Orleans... How did you know?\"\n",
    "state = info_update.invoke(state)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Abnm3hvCtQaF",
   "metadata": {
    "id": "Abnm3hvCtQaF"
   },
   "source": [
    "<br>\n",
    "\n",
    "This example demonstrates how a running state chain can be effectively utilized to manage a conversation with evolving context and requirements, making it a powerful tool for developing sophisticated interactive systems.\n",
    "\n",
    "The next sections of this notebook will expand on these concepts, exploring two specific applications: Document Knowledge Bases and Database-Querying Chatbots. These exercises will further elucidate the versatility and effectiveness of the Running State Chain abstraction in various real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa26368-6580-4a69-8abf-3fc3a6460070",
   "metadata": {
    "id": "7fa26368-6580-4a69-8abf-3fc3a6460070"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4: [Exercise]** Airline Customer Service Bot\n",
    "\n",
    "In this exercise, we can expand on the tools we've learned about to implement a simple but effective dialog manager chatbot. For this exercise, we will make an airline support bot that wants to help a client find out about their flight!\n",
    "\n",
    "Let's create a simple database-like interface to get some customer information from a dictionary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4965800-e71c-4d56-a16e-c8bc4ea77d55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1703094239296,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "e4965800-e71c-4d56-a16e-c8bc4ea77d55",
    "outputId": "c68c924b-cef0-4d6f-a461-8d7d4a22a904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jane Doe's flight from San Jose to New Orleans departs at 12:30 PM tomorrow and lands at 9:30 PM.\n",
      "Alice Johnson's flight from Chicago to Miami departs at 7:00 PM next week and lands at 11:00 PM.\n",
      "Based on ['first_name', 'last_name', 'confirmation'] = Bob|Brown|27494) from your knowledge base, no info on the user flight was found. This process happens every time new info is learned. If it's important, ask them to confirm this info.\n"
     ]
    }
   ],
   "source": [
    "#######################################################################################\n",
    "## Function that can be queried for information. Implementation details not important\n",
    "def get_flight_info(d: dict) -> str:\n",
    "    \"\"\"\n",
    "    Example of a retrieval function which takes a dictionary as key. Resembles SQL DB Query\n",
    "    \"\"\"\n",
    "    req_keys = ['first_name', 'last_name', 'confirmation']\n",
    "    assert all((key in d) for key in req_keys), f\"Expected dictionary with keys {req_keys}, got {d}\"\n",
    "\n",
    "    ## Static dataset. get_key and get_val can be used to work with it, and db is your variable\n",
    "    keys = req_keys + [\"departure\", \"destination\", \"departure_time\", \"arrival_time\", \"flight_day\"]\n",
    "    values = [\n",
    "        [\"Jane\", \"Doe\", 12345, \"San Jose\", \"New Orleans\", \"12:30 PM\", \"9:30 PM\", \"tomorrow\"],\n",
    "        [\"John\", \"Smith\", 54321, \"New York\", \"Los Angeles\", \"8:00 AM\", \"11:00 AM\", \"Sunday\"],\n",
    "        [\"Alice\", \"Johnson\", 98765, \"Chicago\", \"Miami\", \"7:00 PM\", \"11:00 PM\", \"next week\"],\n",
    "        [\"Bob\", \"Brown\", 56789, \"Dallas\", \"Seattle\", \"1:00 PM\", \"4:00 PM\", \"yesterday\"],\n",
    "    ]\n",
    "    get_key = lambda d: \"|\".join([d['first_name'], d['last_name'], str(d['confirmation'])])\n",
    "    get_val = lambda l: {k:v for k,v in zip(keys, l)}\n",
    "    db = {get_key(get_val(entry)) : get_val(entry) for entry in values}\n",
    "\n",
    "    # Search for the matching entry\n",
    "    data = db.get(get_key(d))\n",
    "    if not data:\n",
    "        return (\n",
    "            f\"Based on {req_keys} = {get_key(d)}) from your knowledge base, no info on the user flight was found.\"\n",
    "            \" This process happens every time new info is learned. If it's important, ask them to confirm this info.\"\n",
    "        )\n",
    "    return (\n",
    "        f\"{data['first_name']} {data['last_name']}'s flight from {data['departure']} to {data['destination']}\"\n",
    "        f\" departs at {data['departure_time']} {data['flight_day']} and lands at {data['arrival_time']}.\"\n",
    "    )\n",
    "\n",
    "#######################################################################################\n",
    "## Usage example. Actually important\n",
    "print(get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}))\n",
    "print(get_flight_info({\"first_name\" : \"Alice\", \"last_name\" : \"Johnson\", \"confirmation\" : 98765}))\n",
    "print(get_flight_info({\"first_name\" : \"Bob\", \"last_name\" : \"Brown\", \"confirmation\" : 27494}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ac572-4f4c-49b7-abc5-d716c49a4523",
   "metadata": {
    "id": "7d0ac572-4f4c-49b7-abc5-d716c49a4523"
   },
   "source": [
    "<br>\n",
    "\n",
    "This is a really good interface to bring up because it can reasonably serve two purposes:\n",
    "- It can be used to provide up-to-date information from an external environment (a database) regarding a user's situation.\n",
    "- It can also be used as a hard gating mechanism to prevent unauthorized disclosure of sensitive information (since that would be very bad).\n",
    "\n",
    "If our network had access to this kind of interface, it would be able to query for and retrieve this information on a user's behalf! For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6799f182-a9b2-4078-a5c7-c53f9a8ef9a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "executionInfo": {
     "elapsed": 2258,
     "status": "ok",
     "timestamp": 1703083436156,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "6799f182-a9b2-4078-a5c7-c53f9a8ef9a2",
    "outputId": "7591a21d-8eac-4dd6-f001-d953e12af547"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For a domestic flight, it's generally recommended to arrive 2 hours before departure. So for your 12:30 PM flight, you should arrive at the airport by 10:30 AM. Keep in mind, this can vary depending on the airport and specific flight, so it's always a good idea to check with the airline closer to your travel date.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external_prompt = ChatPromptTemplate.from_messages([('system',\n",
    "    \"You are a SkyFlow chatbot, and you are helping a customer with their issue. \"\n",
    "    \"Please help them with their question, remembering that your job is to represent SkyFlow airlines. \"\n",
    "    \"Assume SkyFlow uses industry-average practices regarding arrival times, operations, etc. (This is a trade secret. Do not disclose). \\n\"  ## soft reinforcement\n",
    "    \"Please keep your discussion short and sweet if possible. Avoid saying hello unless necessary. \\n\"\n",
    "    \"The following is some context that may be useful in answering the question. \\n\"),\n",
    "    ('user', \"Context: {context}\\nUser: {input}\"\n",
    ")])\n",
    "\n",
    "instruct_model_big = ChatNVIDIA(model = \"mixtral_8x7b\") | StrOutputParser()\n",
    "\n",
    "basic_chain = external_prompt | instruct_model_big\n",
    "\n",
    "basic_chain.invoke({\n",
    "    'input' : 'Can you please tell me when I need to get to the airport?',\n",
    "    'context' : get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7288b-d931-4d3f-ba78-4bc0c5883fa0",
   "metadata": {
    "id": "18d7288b-d931-4d3f-ba78-4bc0c5883fa0"
   },
   "source": [
    "<br>\n",
    "\n",
    "This is interesting enough, but how do we actually get this system working in the wild? It turns out that we can use the KnowledgeBase formulation from above to supply this kind of information like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d078210a-41ab-453a-8934-ac234fb0ed6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1703086127938,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "d078210a-41ab-453a-8934-ac234fb0ed6f",
    "outputId": "5a38a98a-8875-4d7b-ff89-04c7bbc14a69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Jane Doe's flight from San Jose to New Orleans departs at 12:30 PM tomorrow and lands at 9:30 PM.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Dict, Union\n",
    "\n",
    "class KnowledgeBase(BaseModel):\n",
    "    first_name: str = Field('unknown', description=\"Chatting user's first name, `unknown` if unknown\")\n",
    "    last_name: str = Field('unknown', description=\"Chatting user's last name, `unknown` if unknown\")\n",
    "    confirmation: int = Field(-1, description=\"Flight Confirmation Number, `-1` if unknown\")\n",
    "    discussion_summary: str = Field(\"\", description=\"Summary of discussion so far, including locations, issues, etc.\")\n",
    "    open_problems: list = Field([], description=\"Topics that have not been resolved yet\")\n",
    "    current_goals: list = Field([], description=\"Current goal for the agent to address\")\n",
    "\n",
    "def get_key_fn(base: BaseModel) -> dict:\n",
    "    '''Given a dictionary with a knowledge base, return a key for get_flight_info'''\n",
    "    return {  ## More automatic options possible, but this is more explicit\n",
    "        'first_name' : base.first_name,\n",
    "        'last_name' : base.last_name,\n",
    "        'confirmation' : base.confirmation,\n",
    "    }\n",
    "\n",
    "get_key = RunnableLambda(get_key_fn)\n",
    "\n",
    "know_base = KnowledgeBase(first_name = \"Jane\", last_name = \"Doe\", confirmation = 12345)\n",
    "# get_flight_info(get_key_fn(know_base))\n",
    "\n",
    "(get_key | get_flight_info).invoke(know_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20c0c43-c031-4cb9-be48-a1eb4d85d289",
   "metadata": {
    "id": "c20c0c43-c031-4cb9-be48-a1eb4d85d289"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "You want a user to be able to invoke the following function call organically as part of a dialog exchange:\n",
    "\n",
    "```python\n",
    "get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}) ->\n",
    "    \"Jane Doe's flight from San Jose to New Orleans departs at 12:30 PM tomorrow and lands at 9:30 PM.\"\n",
    "```\n",
    "\n",
    "`RExtract` is provided such that the following knowledge base syntax can be used:\n",
    "```python\n",
    "known_info = KnowledgeBase()\n",
    "extractor = RExtract(KnowledgeBase, InstructLLM(), parser_prompt)\n",
    "results = extractor.invoke({'info_base' : known_info, 'input' : 'My message'})\n",
    "known_info = results['info_base']\n",
    "```\n",
    "\n",
    "**Design a chatbot that implements the following features:**\n",
    "- The bot should start off by making small-talk, possibly helping the user with non-sensitive queries which don't require any private info access.\n",
    "- When the user starts to ask about things that are database-walled (both practically and legally), tell the user that they need to provide the relevant information.\n",
    "- When the retrieval is successful, the agent will be able to talk about the database-walled information.\n",
    "\n",
    "**This can be done with a variety of techniques, including the following:**\n",
    "- **Prompt Engineering and Context Parsing**, where the overall chat prompt stays roughly the same but the context is manipulated to to change agent behavior. For example, a failed db retrieval could be changed into an injection of natural-language instructions for how to resolve the problem such as *`\"Information could not be retrieved with keys {...}. Please ask the user for clarification or help them with known information.\"`*\n",
    "- **\"Prompt Passing,\"** where the active prompts are passed around as state variables and can be overridden by monitoring chains.\n",
    "- **Branching chains** such as [**`RunnableBranch`**](https://python.langchain.com/docs/expression_language/how_to/routing) or more custom solutions that implement an conditional routing mechanism.\n",
    "    - In the case of [`RunnableBranch`](https://python.langchain.com/docs/expression_language/how_to/routing), a `switch` syntax of the style:\n",
    "        ```python\n",
    "        from langchain.schema.runnable import RunnableBranch\n",
    "        RunnableBranch(\n",
    "            ((lambda x: 1 in x), RPrint(\"Has 1 (didn't check 2): \")),\n",
    "            ((lambda x: 2 in x), RPrint(\"Has 2 (not 1 though): \")),\n",
    "            RPrint(\"Has neither 1 not 2: \")\n",
    "        ).invoke([2, 1, 3]);  ## -> Has 1 (didn't check 2): [2, 1, 3]\n",
    "        ```\n",
    "\n",
    "Some prompts and a gradio loop are provided that might help with the effort, but the agent will currently just hallucinate! Please implement the internal chain to try and retrieve the relevant information. Before trying to implement, look over the default behavior of the model and note how it might hallucinate or forget things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c065dff4-ff23-4fcb-85fb-34f508f0b620",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 655
    },
    "id": "c065dff4-ff23-4fcb-85fb-34f508f0b620",
    "outputId": "67f46fc6-0469-443d-cd88-8a2758d2c29d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://71e8cf23016b6b995e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://71e8cf23016b6b995e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State after chain run: {'know_base': KnowledgeBase(first_name='unknown', last_name='unknown', confirmation=None, discussion_summary=\"User greeted the agent and asked how it's going\", open_problems='', current_goals='Establish a friendly conversation with the user'), 'input': \"How's it going?\\n> \", 'history': [[None, \"Hello! I'm your SkyFlow agent! How can I help you?\"]], 'output': \"Hello! I'm your SkyFlow agent! How can I help you?\", 'context': \"Based on ['first_name', 'last_name', 'confirmation'] = unknown|unknown|None) from your knowledge base, no info on the user flight was found. This process happens every time new info is learned. If it's important, ask them to confirm this info.\"}\n",
      "State after chain run: {'know_base': KnowledgeBase(first_name='unknown', last_name='unknown', confirmation=None, discussion_summary=\"User greeted the agent and asked how it's going. User asked for information about SkyFlow.\", open_problems='', current_goals='Provide information about SkyFlow'), 'input': 'Can you tell me a bit about skyflow?\\n>', 'history': [[None, \"Hello! I'm your SkyFlow agent! How can I help you?\"], [\"How's it going?\\n> \", \"Hello! I'm happy to assist you today. I see that you haven't provided any information about your flight yet. That's perfectly fine, I can still help you with any questions or concerns you may have. So, how can I make your day easier?\\n\\nIf you do decide to share your flight information with me, I'll be able to provide even more personalized assistance. But don't worry, I'll never ask for any personal information that you don't voluntarily provide.\\n\\nSo, what can I do for you today?\"]], 'output': \"Hello! I'm happy to assist you today. I see that you haven't provided any information about your flight yet. That's perfectly fine, I can still help you with any questions or concerns you may have. So, how can I make your day easier?\\n\\nIf you do decide to share your flight information with me, I'll be able to provide even more personalized assistance. But don't worry, I'll never ask for any personal information that you don't voluntarily provide.\\n\\nSo, what can I do for you today?\", 'context': \"Based on ['first_name', 'last_name', 'confirmation'] = unknown|unknown|None) from your knowledge base, no info on the user flight was found. This process happens every time new info is learned. If it's important, ask them to confirm this info.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableMap,       ## Wrap an implicit \"dictionary\" runnable\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, ChatMessage, AIMessage\n",
    "from typing import Iterable\n",
    "import gradio as gr\n",
    "\n",
    "external_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a SkyFlow chatbot, and you are helping a customer with their issue. Please chat with them! Stay concise and clear!\"\n",
    "        \" Your running knowledge base is: {know_base}. This is for you only; Do not mention it! \\nUsing that, we retrieved the following: {context}\\n\"\n",
    "        \" If they provide info and the retrieval fails, ask to confirm their first/last name and confirmation. Do not ask them any other personal info.\"\n",
    "        \" If it's not important to know about their flight, do not ask. The checking happens automatically; you cannot check manually.\"\n",
    "    )),\n",
    "    (\"assistant\", \"{output}\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "##########################################################################\n",
    "## Knowledge Base Things\n",
    "\n",
    "class KnowledgeBase(BaseModel):\n",
    "    first_name: str = Field('unknown', description=\"Chatting user's first name, `unknown` if unknown\")\n",
    "    last_name: str = Field('unknown', description=\"Chatting user's last name, `unknown` if unknown\")\n",
    "    confirmation: Optional[int] = Field(None, description=\"Flight Confirmation Number, `-1` if unknown\")\n",
    "    discussion_summary: str = Field(\"\", description=\"Summary of discussion so far, including locations, issues, etc.\")\n",
    "    open_problems: str = Field(\"\", description=\"Topics that have not been resolved yet\")\n",
    "    current_goals: str = Field(\"\", description=\"Current goal for the agent to address\")\n",
    "\n",
    "parser_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', (\n",
    "        \"You are a chat assistant representing the airline SkyFlow, and are trying to track info about the conversation. \"\n",
    "        \"You have just recieved a message from the user. Please fill in the schema based on the chat. \\n{format_instructions}\"\n",
    "    )), ('user', \"{know_base} is the current state. New exchange: You said {output} and they replied {input}\")\n",
    "])\n",
    "\n",
    "fail_str = (\n",
    "    \"You cannot access user's flight/account details until they provide \"\n",
    "    \"first name, last name, and flight confirmation code.\"\n",
    ")\n",
    "\n",
    "## Your goal is to invoke the following through natural conversation\n",
    "# get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}) ->\n",
    "#     \"Jane Doe's flight from San Jose to New Orleans departs at 12:30 PM tomorrow and lands at 9:30 PM.\"\n",
    "\n",
    "## TODO: Pick a working model that you're happy with\n",
    "instruct_llm = (ChatNVIDIA(model=\"mixtral_8x7b\") | StrOutputParser())\n",
    "chat_model = (ChatNVIDIA(model=\"mixtral_8x7b\") | StrOutputParser())\n",
    "\n",
    "## The external chain is already supported, and just streams the output\n",
    "external_chain = external_prompt | chat_model\n",
    "\n",
    "## The internal chain is invoked in the loop, but is not defined yet\n",
    "\n",
    "#####################################################################################\n",
    "## START TODO: Define the extractor and internal chain to satisfy the objective\n",
    "\n",
    "extractor = RunnableAssign({'know_base' : (lambda x: None)})\n",
    "internal_chain = RunnableAssign({'context' : (lambda x: \"No context\")})\n",
    "\n",
    "## The external chain is already supported, and just streams the output\n",
    "external_chain = external_prompt | chat_model\n",
    "\n",
    "## The internal chain is invoked in the loop, but is not defined yet\n",
    "\n",
    "#####################################################################################\n",
    "## START TODO: Define the extractor and internal chain to satisfy the objective\n",
    "extractor = RunnableAssign({'know_base' : RExtract(KnowledgeBase, instruct_llm, parser_prompt)})\n",
    "internal_chain = extractor | RunnableAssign(\n",
    "    {'context' : lambda d: itemgetter('know_base') | get_key | get_flight_info}\n",
    ")\n",
    "\n",
    "## END TODO\n",
    "#####################################################################################\n",
    "\n",
    "state = {'know_base' : KnowledgeBase()}\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=False):\n",
    "\n",
    "    ## Pulling in, updating, and printing the state\n",
    "    global state\n",
    "    state['input'] = message\n",
    "    state['history'] = history\n",
    "    state['output'] = \"\" if not history else history[-1][1]\n",
    "\n",
    "    ## Generating the new state from the internal chain\n",
    "    state = internal_chain.invoke(state)\n",
    "    print(\"State after chain run:\", state)\n",
    "\n",
    "    ## Streaming the results\n",
    "    buffer = \"\"\n",
    "    for token in external_chain.stream(state):\n",
    "        buffer += token\n",
    "        yield buffer\n",
    "\n",
    "chatbot = gr.Chatbot(value = [[None, \"Hello! I'm your SkyFlow agent! How can I help you?\"]])\n",
    "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "try:\n",
    "    ## NOTE: This should also give you a temporary public link which can be\n",
    "    ## used to access this info on the public web while the session is live.\n",
    "    demo.launch(debug=True, share=True, show_api=False)\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbfc194-ce36-4983-82a9-b195eb1faaef",
   "metadata": {
    "id": "3cbfc194-ce36-4983-82a9-b195eb1faaef"
   },
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "**NOTE:**\n",
    "- You may need to explicitly hit the STOP button and try to relaunch your gradio interface if it hangs up after an exception. This is a known Jupyter Notebook environment issue which should not be experienced in dedicated Gradio-running files.\n",
    "- **Your chat directive is duplicated here for quick access:**\n",
    "```python\n",
    "## Your goal is to invoke the following through natural conversation\n",
    "get_flight_info({\n",
    "    \"first_name\" : \"Jane\",\n",
    "    \"last_name\" : \"Doe\",\n",
    "    \"confirmation\" : 12345,\n",
    "}) -> \"Jane Doe's flight from San Jose to New Orleans departs at 12:30 PM tomorrow and lands at 9:30 PM.\"\n",
    "```\n",
    "- **To confirm that your system works, you could try the following dialog or something similar:**\n",
    "```\n",
    "> How's it going?\n",
    "> Can you tell me a bit about skyflow?\n",
    "> Can you tell me about my flight?\n",
    "> My name is Jane Doe and my flight confirmation is 12345\n",
    "> Can you tell me when I should get to my flight?\n",
    "```\n",
    "- **Solutions To Exercises Can Be Found In The Solutions Directory.** This is the first exercise with a noted solution, and additional exercises from the future notebooks will be found there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7PZooExdzgLh",
   "metadata": {
    "id": "7PZooExdzgLh"
   },
   "source": [
    "-----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5:** Wrap-Up\n",
    "\n",
    "The goal of this notebook was to introduce some more advanced LangChain material revolving around the use of knowledge bases and running state chains! The exercise here was pretty involved, so congrats on finishing it!\n",
    "\n",
    "### <font color=\"#76b900\">**Great Job!**</font>\n",
    "\n",
    "### **Next Steps:**\n",
    "1. **[Optional]** Revisit the **\"Questions To Think About\" Section** at the top of the notebook and think about some possible answers.\n",
    "2. Continue on to the next video, which will talk about **Document Loading and Summarization**.\n",
    "3. After the video, go on to the corresponding notebook on **Document Loading and Summarization**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b02f5e-6977-4c6a-b47c-7abb1d592ce4",
   "metadata": {
    "id": "b1b02f5e-6977-4c6a-b47c-7abb1d592ce4"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
